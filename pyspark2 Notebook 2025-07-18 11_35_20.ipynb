{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e37a844-148f-43f6-8b64-d50cd9fa5e52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----------+------+\n| ID|   Name| Department|Salary|\n+---+-------+-----------+------+\n|  1|  Alice|Engineering| 65000|\n|  2|    Bob|  Marketing| 58000|\n|  3|Charlie|      Sales| 52000|\n|  4|  David|Engineering| 72000|\n|  5|    Eve|      Sales| 54000|\n+---+-------+-----------+------+\n\nroot\n |-- ID: long (nullable = true)\n |-- Name: string (nullable = true)\n |-- Department: string (nullable = true)\n |-- Salary: long (nullable = true)\n\n+---+-----+-----------+------+\n| ID| Name| Department|Salary|\n+---+-----+-----------+------+\n|  1|Alice|Engineering| 65000|\n|  4|David|Engineering| 72000|\n+---+-----+-----------+------+\n\n+-----------+-----+\n| Department|count|\n+-----------+-----+\n|Engineering|    2|\n|  Marketing|    1|\n|      Sales|    2|\n+-----------+-----+\n\n+-----------+-----------+\n| Department|avg(Salary)|\n+-----------+-----------+\n|Engineering|    68500.0|\n|  Marketing|    58000.0|\n|      Sales|    53000.0|\n+-----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"StudentAssignment\").getOrCreate()\n",
    "# Sample employee data\n",
    "data = [\n",
    " (1, \"Alice\", \"Engineering\", 65000),\n",
    " (2, \"Bob\", \"Marketing\", 58000),\n",
    " (3, \"Charlie\", \"Sales\", 52000),\n",
    " (4, \"David\", \"Engineering\", 72000),\n",
    " (5, \"Eve\", \"Sales\", 54000)\n",
    "]\n",
    "schema = [\"ID\", \"Name\", \"Department\", \"Salary\"]\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "df.show()\n",
    "#Show schema\n",
    "df.printSchema()\n",
    "# Filter: Salary > 60000\n",
    "df.filter(df[\"Salary\"] > 60000).show()\n",
    "# Group by Department\n",
    "df.groupBy(\"Department\").count().show()\n",
    "# Average Salary by Department\n",
    "df.groupBy(\"Department\").avg(\"Salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cef0b548-0133-4dd5-914a-71d8b8f59c9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+--------+------------+---------+----------+------+\n|        _c0|       _c1|      _c2|     _c3|         _c4|      _c5|       _c6|   _c7|\n+-----------+----------+---------+--------+------------+---------+----------+------+\n|EMPLOYEE_ID|FIRST_NAME|LAST_NAME|   EMAIL|PHONE_NUMBER|HIRE_DATE|    JOB_ID|SALARY|\n|        198|    Donald| OConnell|DOCONNEL|650.507.9833|21-Jun-07|  SH_CLERK|  2600|\n|        199|   Douglas|    Grant|  DGRANT|650.507.9844|13-Jan-08|  SH_CLERK|  2600|\n|        200|  Jennifer|   Whalen| JWHALEN|515.123.4444|17-Sep-03|   AD_ASST|  4400|\n|        201|   Michael|Hartstein|MHARTSTE|515.123.5555|17-Feb-04|    MK_MAN| 13000|\n|        202|       Pat|      Fay|    PFAY|603.123.6666|17-Aug-05|    MK_REP|  6000|\n|        203|     Susan|   Mavris| SMAVRIS|515.123.7777|07-Jun-02|    HR_REP|  6500|\n|        204|   Hermann|     Baer|   HBAER|515.123.8888|07-Jun-02|    PR_REP| 10000|\n|        205|   Shelley|  Higgins|SHIGGINS|515.123.8080|07-Jun-02|    AC_MGR| 12008|\n|        206|   William|    Gietz|  WGIETZ|515.123.8181|07-Jun-02|AC_ACCOUNT|  8300|\n|        100|    Steven|     King|   SKING|515.123.4567|17-Jun-03|   AD_PRES| 24000|\n|        101|     Neena|  Kochhar|NKOCHHAR|515.123.4568|21-Sep-05|     AD_VP| 17000|\n|        102|       Lex|  De Haan| LDEHAAN|515.123.4569|13-Jan-01|     AD_VP| 17000|\n|        103| Alexander|   Hunold| AHUNOLD|590.423.4567|03-Jan-06|   IT_PROG|  9000|\n|        104|     Bruce|    Ernst|  BERNST|590.423.4568|21-May-07|   IT_PROG|  6000|\n|        105|     David|   Austin| DAUSTIN|590.423.4569|25-Jun-05|   IT_PROG|  4800|\n|        106|     Valli|Pataballa|VPATABAL|590.423.4560|05-Feb-06|   IT_PROG|  4800|\n|        107|     Diana|  Lorentz|DLORENTZ|590.423.5567|07-Feb-07|   IT_PROG|  4200|\n|        108|     Nancy|Greenberg|NGREENBE|515.124.4569|17-Aug-02|    FI_MGR| 12008|\n|        109|    Daniel|   Faviet| DFAVIET|515.124.4169|16-Aug-02|FI_ACCOUNT|  9000|\n+-----------+----------+---------+--------+------------+---------+----------+------+\nonly showing top 20 rows\nroot\n |-- _c0: string (nullable = true)\n |-- _c1: string (nullable = true)\n |-- _c2: string (nullable = true)\n |-- _c3: string (nullable = true)\n |-- _c4: string (nullable = true)\n |-- _c5: string (nullable = true)\n |-- _c6: string (nullable = true)\n |-- _c7: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Spark DataFrames\").getOrCreate()\n",
    "df = spark.read.csv(\"/Volumes/pyspark7/default/pyspark1/employees1.csv\")\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c54a602-8802-4882-abb8-9b36af4c5278",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9c8a562-ade1-4463-a847-3cbd2aecd981",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.option(\"header\", \"true\").csv(\"/Volumes/pyspark7/default/pyspark1/employees1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7754bfa-483f-4ab7-ad9a-9104a7763e2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "# Limit rows for display\n",
    "pandas_df = df.limit(20).toPandas()\n",
    "\n",
    "# Print boxed table\n",
    "print(tabulate(pandas_df, headers='keys', tablefmt='grid'))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "231033ba-26ec-4546-b8d7-e7bcfcf9a429",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>JOB_ID</th><th>avg(SALARY)</th></tr></thead><tbody><tr><td>PU_CLERK</td><td>2780.0</td></tr><tr><td>PU_MAN</td><td>11000.0</td></tr><tr><td>AD_VP</td><td>17000.0</td></tr><tr><td>AC_MGR</td><td>12008.0</td></tr><tr><td>AD_PRES</td><td>24000.0</td></tr><tr><td>ST_MAN</td><td>7280.0</td></tr><tr><td>IT_PROG</td><td>5760.0</td></tr><tr><td>HR_REP</td><td>6500.0</td></tr><tr><td>FI_ACCOUNT</td><td>7920.0</td></tr><tr><td>AD_ASST</td><td>4400.0</td></tr><tr><td>PR_REP</td><td>10000.0</td></tr><tr><td>MK_REP</td><td>6000.0</td></tr><tr><td>AC_ACCOUNT</td><td>8300.0</td></tr><tr><td>FI_MGR</td><td>12008.0</td></tr><tr><td>MK_MAN</td><td>13000.0</td></tr><tr><td>ST_CLERK</td><td>2750.0</td></tr><tr><td>SH_CLERK</td><td>2600.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "PU_CLERK",
         2780.0
        ],
        [
         "PU_MAN",
         11000.0
        ],
        [
         "AD_VP",
         17000.0
        ],
        [
         "AC_MGR",
         12008.0
        ],
        [
         "AD_PRES",
         24000.0
        ],
        [
         "ST_MAN",
         7280.0
        ],
        [
         "IT_PROG",
         5760.0
        ],
        [
         "HR_REP",
         6500.0
        ],
        [
         "FI_ACCOUNT",
         7920.0
        ],
        [
         "AD_ASST",
         4400.0
        ],
        [
         "PR_REP",
         10000.0
        ],
        [
         "MK_REP",
         6000.0
        ],
        [
         "AC_ACCOUNT",
         8300.0
        ],
        [
         "FI_MGR",
         12008.0
        ],
        [
         "MK_MAN",
         13000.0
        ],
        [
         "ST_CLERK",
         2750.0
        ],
        [
         "SH_CLERK",
         2600.0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "JOB_ID",
         "type": "\"string\""
        },
        {
         "metadata": "{\"__autoGeneratedAlias\": \"true\"}",
         "name": "avg(SALARY)",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = df.withColumn(\"SALARY\", col(\"SALARY\").cast(\"double\"))\n",
    "df.groupBy(\"JOB_ID\").avg(\"SALARY\").show()\n",
    "df = df.withColumn(\"Bonus\", df.SALARY * 0.10)\n",
    "df.show()\n",
    "\n",
    "df.filter(df.SALARY > 70000).show()\n",
    "\n",
    "# Assuming you want to group by JOB_ID instead of Department\n",
    "df.groupBy(\"JOB_ID\").avg(\"SALARY\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d573955f-d04c-42de-93eb-6e709b83b9fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>JOB_ID</th><th>avg(SALARY)</th></tr></thead><tbody><tr><td>PU_CLERK</td><td>2780.0</td></tr><tr><td>PU_MAN</td><td>11000.0</td></tr><tr><td>AD_VP</td><td>17000.0</td></tr><tr><td>AC_MGR</td><td>12008.0</td></tr><tr><td>AD_PRES</td><td>24000.0</td></tr><tr><td>ST_MAN</td><td>7280.0</td></tr><tr><td>IT_PROG</td><td>5760.0</td></tr><tr><td>HR_REP</td><td>6500.0</td></tr><tr><td>FI_ACCOUNT</td><td>7920.0</td></tr><tr><td>AD_ASST</td><td>4400.0</td></tr><tr><td>PR_REP</td><td>10000.0</td></tr><tr><td>MK_REP</td><td>6000.0</td></tr><tr><td>AC_ACCOUNT</td><td>8300.0</td></tr><tr><td>FI_MGR</td><td>12008.0</td></tr><tr><td>MK_MAN</td><td>13000.0</td></tr><tr><td>ST_CLERK</td><td>2750.0</td></tr><tr><td>SH_CLERK</td><td>2600.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "PU_CLERK",
         2780.0
        ],
        [
         "PU_MAN",
         11000.0
        ],
        [
         "AD_VP",
         17000.0
        ],
        [
         "AC_MGR",
         12008.0
        ],
        [
         "AD_PRES",
         24000.0
        ],
        [
         "ST_MAN",
         7280.0
        ],
        [
         "IT_PROG",
         5760.0
        ],
        [
         "HR_REP",
         6500.0
        ],
        [
         "FI_ACCOUNT",
         7920.0
        ],
        [
         "AD_ASST",
         4400.0
        ],
        [
         "PR_REP",
         10000.0
        ],
        [
         "MK_REP",
         6000.0
        ],
        [
         "AC_ACCOUNT",
         8300.0
        ],
        [
         "FI_MGR",
         12008.0
        ],
        [
         "MK_MAN",
         13000.0
        ],
        [
         "ST_CLERK",
         2750.0
        ],
        [
         "SH_CLERK",
         2600.0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "JOB_ID",
         "type": "\"string\""
        },
        {
         "metadata": "{\"__autoGeneratedAlias\": \"true\"}",
         "name": "avg(SALARY)",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Step 1: Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"EmployeeDataAnalysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Step 2: Load CSV File\n",
    "df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"/Volumes/pyspark7/default/pyspark1/employees1.csv\")\n",
    "df.show()\n",
    "\n",
    "# Step 3: View Schema\n",
    "df.printSchema()\n",
    "\n",
    "# Step 4: Analyze Salaries by Job ID\n",
    "df.groupBy(\"JOB_ID\").avg(\"SALARY\").show()\n",
    "\n",
    "# Step 5: Add Bonus Column (10% of salary)\n",
    "from pyspark.sql.functions import col\n",
    "df = df.withColumn(\"Bonus\", col(\"SALARY\") * 0.10)\n",
    "df.show()\n",
    "\n",
    "# Step 6: Filter High Earners (>70,000)\n",
    "df.filter(col(\"SALARY\") > 70000).show()\n",
    "\n",
    "# Step 7: Visualize in Databricks\n",
    "df.groupBy(\"JOB_ID\").avg(\"SALARY\").display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "pyspark2 Notebook 2025-07-18 11:35:20",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}